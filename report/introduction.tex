\chapter{Introduction}
\label{chap:In}
 \THsec{Motivation}{motivation}
We are always in need of faster and smaller computers that burn less energy. Over
the second half of the twentieth century, the speed of our CPUs (central processing units)
was increased by letting them run at higher and higher frequencies. However, a linear
frequency speed-up causes an exponential increase in energy and heat. In fact, a rule of thumb
is that you are spending as much energy as needed for the computation to cool the
circuit. Since it has become desirable to make smaller and smaller units, it becomes harder and harder
to cool the circuits down. However, software developers still
require more and more of the hardware. When CPUs cannot speed-up
in the same way as before, the CPUs needs to become more effective. That is why
pipelining was introduced into the CPUs. For example, if one instruction takes six seconds to compute, then two instructions will be completed after twelve seconds. If the work were to be divided into three stages where one instruction spends two seconds in each stage, the CPU can then begin working on a new instruction every two seconds. Given this improvement, four instructions are completed in twelve seconds.
Twice the amount of work can be done during a period without increasing the speed
of the CPU and therefore the energy consumption roughly remains the same.
\\ \\
Another opportunity to take advantage of is that computing some instructions, especially
memory instructions (loads and stores) involves long waiting times. If, instead of waiting, other work is computed with the next instructions, one are not affected by the waiting time ultimately. Because the computation of the instructions next in line is started while waiting, this is called out of order execution. It is up to the CPU to decide if the work can be done in another order and still produce the same outcome. 
\\ \\
Like mentioned above computation of memory instructions includes waiting for write permission and that useful tasks can be done while waiting. Still, it is advantageous to decrease the waiting time since there might not always be useful tasks to work on that cover the entire waiting time. This thesis aims to investigate whether we can shorten the waiting time by prefetching the data needed for store instructions in advance. The earlier a data permission is granted, the earlier it is ready to use.
 \THsec{Scope}{scope}
This master thesis introduces and evaluates the three state-of-the-art policies (see \ref{subsec:GPP}) for prefetching permission for the store instruction. There has not been much research focusing on the acceleration of store instructions in particular. The three state-of-the-art policies (OnExecure, OnCommit, and NoPrefetch) are going to be combined in different ways to try to come up with a more optimal policy concerning speed up (number of cycles), number of prefetches, L1 accesses, and power consumption. Furthermore, some of the preparation work concerning editing and understanding traces from benchmark programs are covered as well. 
 \THsec{Related work}{relWork}
The related work for this master thesis is covered in subsection~\ref{subsec:GPP} were the state-of-the-art store prefetch policies are described. 


\THsec{Structure of the report}{structure}

This report consists of eight chapters. 
\paragraph{Chapter \ref{chap:In} - Introduction} This gives a motivation of the work along with its scope. 
\paragraph{Chapter \ref{chap:bg} - Background} This chapter is divided into two parts. The first part covers
the used CPU architecture and related terms to be used throughout the report along with introducing the state-of-the-art policies. The second part covers the methodology employed in this thesis.
\paragraph{Chapter \ref{chap:SettingUpTheTestbed} - Setting Up The Testbed} This chapter can be seen as a continuation of the
second part in chapter \ref{chap:bg}.  Here the modification of existing simulation tools are covered. The trace employed to connect the simulation infrastructure is covered in this chapter.
\paragraph{Chapter \ref{chap:ProposedPrefetchPolicies} - Proposed Store Prefetch Policies} This chapter introduces all store
 prefetch policies that are proposed within the work of this thesis.
\paragraph{Chapter \ref{chap:results} - Results} In this chapter includes graphs comparing the different
 policies with different settings concerning execution times, L1 accesses, useful prefetch, and energy.
\paragraph{Chapter \ref{chap:discussion} - Discussion} Issues with the set up that can have an impact on the results are covered in this chapter.
\paragraph{Chapter \ref{chap:conclusion} - Conclusions} This chapter will offer conclusions about the state-of-the-art and proposed store prefetch policies.
\paragraph{Chapter \ref{chap:fetueredWork} - Future work} This chapter presents ideas on future work that can be
built upon the work of this thesis.
