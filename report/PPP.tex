\chapter{Proposed Store Prefetch Policies}
\label{chap:ProposedPrefetchPolicies}
This chapter will introduce the new store prefetch policies proposed by this thesis. Three different set of techniques for aiming to decrease the runtime and energy consumption are represented by each section below.
\\ \\
From now on the terms policy and filter are used. A policy is a store prefetch policy while a filter is a less complex policy that can more easily be added on top of another policy, which will prevent some prefetches from occurring at all.
\section{Techniques to reduce speculation effect} 
This techniques aim to reduce the speculation effect by not prefetching permission to stores that may not be committing or that has already recently been. In the last case, prefetch may not be need since data are likely to still be in L1.
\subsection{OnNonBranchSpecultive}
If you have a store instruction that follows a branch, it is better to wait until the outcome of the branch is known, to make
sure that all data we bring in to the L1 cache will be needed. If
the load is not affected by a branch, we will benefit from the performance by an early prefetch. The difference between OnNonBSpeculative and OnCommit
is that the prefetch will, if not affected by a branch, be issued upon arriving in the
ROB. Compare to OnCommit we earn the time the instruction is waiting in the ROB.
\subsection{Re-Execute}
Re-Execute is not a fully prefetch policy. It is more of a question on how to handle Re-Execution of an instruction. 
 Reasons for Re-Execution can
either be that a load follows a store with an unknown address that is later computed to
the same as the store. In this case, the load has to be Re-Executed to get the data that
is updated by the previous store. Another reason can be that the load in question has
a previously not resolved load. If the load is invalidated or evicted from the cache,
then the load (and all the next instructions) have to be Re-Executed.
\\ \\
Here a comparison is made between issue a prefetch on ReExececution
or not. If the Re-Execution takes place close in time to the first execution, then the data
for that store is likely to be in L1 and/or in another cache, hence prefetching again
is not necessary and thereby just a waste of energy.
\section{Techniques to filter unnecessary prefetches}
Here the memory address of the store is evaluated. What are our experience from handling stores from this address previously, were their a need of prefetch or not? Another thing to be considered is if the data is sharing a cache line with data that has already been prefetched. Since the entire cache line is always prefetched. 
 \subsection{SamecacheeLine}
When retrieving data from main memory to a cache, a chunk of adjacent fields is read into the cache. The reason for this is since the same data (i.e., the contents of a file) is usually stored next to each other in memory, making this chunk of data (called cache line) more likely to contain the next data that will be requested. The entire cache line has the same physical address. SameCacheLine is like Re-Execute a primitive policy that can be called a filter. It has a register that keeps the previous address so that the current one can be checked against it for equality. If there is an equality and it has already been prefetch (given it should according to the policy used in combination with SameCacheLine) it, therefore, does not need to be prefetch again.  
 \subsection{PCbasedPredictor}
The idea pf PCbasedPredictor comes from branch prediction, which given the previous observations on whether a branch is taken or not predicts if it is going to be taken or not. The prediction is based on information on the target PC (the address of the target instruction), and whether or not branches to this instruction are usually taken or not. If it is likely that a branch is to be taken, then the predictor predicts that it is going to be taken this time as well and the CPU is fed whit instructions from the target of the branch. If it is predicted that a branch is not to be taken the CPU gets the instructions that come after the branch. \\ \\
PCbasedPredictor keeps track of the memory addresses of the store instructions instead. PCbasedPredictor uses a buffer (as you soon will see different buffer sizes have been tested) in which each entry map to a number of memory addresses. The more entries, the fewer addresses map to it. The rest you get when dividing the address (interpreted as a number) with the number of entries in the index to the buffer. That means that two adjacent memory addresses maps to different entries.\\ \\


Every entry contains an integer that can take values between 0 and 3 and is preset to 2. If seeing a store to a memory address that maps to an entry with a number greater than one, the data will be prefetch. If a store instruction hits in the L1 cache (the data was already there) the number in the corresponding buffer entry is (if it is not 0) decreased by one, making it less likely to be prefetch next time. If a store misses in the L1 cache (the data is not yet there), the number in the corresponding buffer entry is (if it is not 3) increased by one, making it more likely to be prefetch next time. \\ \\


To evaluate the impact of the buffer size, different sizes were tested and named PCNecessery X, where X to the power of two gives the number of entries.



\section{Techniques to adapt to timeliness} 
Techniques for timeliness focus not on if, but when to prefetch. An early prefetch might be evicted other prefetch but not yet used data due to space issues. To avoid this issues, the prefetch can be delayed, so the data to be evicted can be used before eviction. 
\subsection{PCbasedTimeLinessPredictor}

PCbasedTimeLinessPredictor uses two buffers, where one of the buffers is the one used in PCbasedPredictor and the other keeps track of when to issue the prefetch, this is called the Timeliness buffer. The Timeliness buffer is implemented in the same way (with an integer between 0 and 3 and the same mapping to the data address of the store) as the one in PCbasedPredictor. The second buffer stores information on what type of miss it is. A late miss means that data is not ready in L1 when it's needed, in this case, the number is increased by one (if it is not 3). An early miss means that the data has arrived early to the L1 cache and has been evicted by data that was prefetched later to the same location in the L1 cache. In the same way that an entry in one of their buffer is statically mapped to more than a memory addresses a cache has a limited number of slots for every address. If all the places are taken, it might (depending on the replacement policy in place) replace the old but not yet used data. An early miss decreases (if it is not 0) the corresponding value in the Timeliness buffer. This policy is the only one that does not only try to predict if data should be prefetched or not but also when.  \\ \\

When deciding on if and when to prefetch, the first buffer is used in the same way as PCbasedPredictor to predict if a prefetch should be done or not. If it will be prefetch, the value of the Timeliness buffer is checked to decide on when. A value below Z means that it will be prefetch later, like OnCommit. A value above or equal to Z will cause a later prefetch, like  OnNonBranchSpecultive. Different values on Z (2 and 3) are implemented together with different sizes of the two buffers (they have all the same size on their two buffers). The versions are named PCbasedTimeLinessPredictorZ X where X denotes the size of the buffers in the same way as for PCbasedPredictor. 

\section{General remarks}
Each new policy and filter can be built on top of each other to gain better performance. In the result (chapter \ref{chap:results}) you can see how filters and policies have been used on top of each other.
