\chapter{Results}
\label{chap:results}
This chapter shows the results of the different filters and store prefetch policies concerning; execution times, number of L1 accesses, number of store prefetch, and energy consumption, these are referred to as parameters. This chapter is divided in four section on for each set of techniques (introduced in chapter \ref{chap:ProposedPrefetchPolicies}) and the State-of-the-art once (see \ref{subsec:GPP}). In each of this sections, there is one subsection for each parameter. Every subsection presents the average values for each compared policy combination in a table. This table follows by a graph with the values of all benchmark for all compared policy combination. The values are given in percentages weighted against OnCommit. Finally we percent the five benchmarks that befit the most and the least from a certain policy combination compared with OnCommit. 
\THsec{State-of-the-art policies}{base}

First, we will analyze the following prefetch policies:
\begin{itemize}
    \item NoPrefetch
    \item OnExecute
    \item OnCommit
\end{itemize}
\resExtime
\avgTable{extime}{run1-0-avg}{execution time}{}
This table concludes the impact the runtime has had of prefetch. When using any type of prefetch policy will roughly cut the runtime with a third (for OnCommit and OnExecute). These results tell us once and for all that the bringing on data into the L1 cache for store instruction takes time. Still, it is not just to prefetch however aggressively you want. If we compare OnExecute and OnCommit, we can see that OnExecute is 2.35 percent units slower. Since OnExecute issues their prefetch earlier they will, therefore, prefetch more data that later will turn out to not be used. Because unnecessary prefetch slows down the execution, it, therefore, be minimized.
\fullTable{extime}{run1-0-full}{Execution time}
\toplist{extime}{run1-0}{execution time}{OnExexcute}
In the table above you can see the number of percent units in times of the number of cycles for a particular benchmark when comparing OnExecute and OnCommit. Gcc 3 and Gobmk 4 gain over 20 percent in reduced number of cycles while running OnCommit (being less aggressive with the prefetch). On the other hand, OnCommit is increasing the number of cycles for omnetpp and castusADM with a twenty percent unit. Given the provided measurements, here and in the following subsections, it is hard to tell why a certain benchmark gains or loses performance (reduced number of cycles). I believe that this can be explained as to how predictable the execution is, a predictable benchmark will be helped by an early prefetch, for example, OnExecute. However, a less predictable benchmark, using OnExecute, will stall the CPU by filling up the store buffer.
\resAcc

\avgTable{L1}{run1-1-avg}{number of L1 accesses}{}
This is one interesting graph which shows the amount of unnecessary work caused by prefetching. NoPrefetch does, as the name inclines, no prefetches which mean that all L1 accesses are caused by needed data for store instructions that are to be executed to the point in time when the need occurs. Everything above 84.82\% is unnecessary work and will waste energy. OnExecute does as expected more accesses than OnCommit since it prefetches earlier and more speculative.

\fullTable{L1}{run1-1-full}{Number of L1 accesses}
\toplist{L1_1}{run1-1}{number of L1 accesses}{OnExexcute}
The table shows us that OnCommit decreases the L1 accesses for all but two of the benchmarks and the difference can be over 90 percent units (bzip2 1 and bizp2 2). This behavior shows once again that an early prefetch trigger unneeded accesses to the L1 cache. It is interesting that spolex has more accesses for OnExecute then OnCommit and this might be something to investigate further.  
\resSp
\avgTable{sp}{run1-2-avg}{number of store prefetches}{}
The number of store prefetches confirms the behaviors and conclusions drawn from the tables over execution times and L1 accesses. NoPrefetch does no store prefetch while OnExeute does most. OnCommit lays in between, 299.72 percent units below OnExecute. This is also what to expect since OnExecute is more speculative then OnCommit.  
\fullTable{sp}{run1-2-full}{number of store prefetches}
\toplist{sp1}{run1-2}{number of store prefetches}{OnExexcute}
OnExecute issues more store prefetches then OnCommit for all benchmarks except bwaves which has the same number of prefetches for OnCommit and OnExecute. It is hard to explain why bwaves issue more store prefetches on OnCommit.
\resEnergy
\avgTable{energy}{run1-3-avg}{energy consumption}{}
This graph shows how much we need to pay concerning energy for the cuts of cycles. First of all, we see that NoPrefetch consumes the smallest amount of energy while OnExecute consumes most. This consumption means that we are paying for unnecessary prefetches. It is also worth noting that if using OnExecute instead of OnCommit we decrease the number of cycles by 2.35 percent units, but the energy consumption increases with 33.13 percent units. Using OnCommit instead of NoPrefetch burns 9.05 percent units more energy but that gives a speedup of 46.5 percent units. A prefetch policy can pay off in decreased energy consumption, but if getting too far in prefetching the loss concerning increased energy consumption can be huge.
\fullTable{energy}{run1-3-full}{energy consumption}
\toplist{energy}{run1-3}{energy consumption}{OnExexcute}
Using OnCommit rather then OnExexute saves much energy for many benchmarks. Finding Bizp2 in the lead when it comes saving energy is no surprise since it is in the lead when it comes to decreasing prefetches and L1 accesses. This correlation will also explain why Milc increase a bit in energy consumption while using OnCommit instead of OnExecute. Longer execution time will also increase the energy consumption.

\subsection{Conclusion}
All three prefetch policies are not a result of this thesis, and therefore all will be kept as a reference when now adding more filters on top of this to be more specific in when to prefetch or not.

\THsec{Techniques to reduce speculation effect}{reduce}
Second, we will analyze all prefetch policies from state of the art and the following three new once:
\begin{itemize}
    \item OnNonBSpec
    \item OnExecute with Re-Execute
    \item OnNonBSpec with Re-Execute
\end{itemize}
\resExtime
\avgTable{extime2}{run2-0-avg}{execution time}{}
OnNonBSpec seems to be the fastest one, 3.34\% faster then OnCommit. It makes a difference not to prefetch stores that are affected by a branch. The branch predictions seems to be false in most cases. We can also see that the ReExecute filter does not affect either OnExecute nor OnNonBSpec.
\fullTable{extime2}{run2-0-full}{Execution time}
\toplist{extime2}{run2-0}{execution time)}{OnNonBSpec with Re-Execute}
The benchmarks that befit the most from OnNonBSpec with Re-Execution does so with just 1.91 percent units and the one that loses most, losses over 10 percent units (cactusADM and omnetpp). A guess is that the variation between the benchmark is the number of stores that are affected by a branch.
\resAcc
\avgTable{L12}{run2-1-avg}{number of L1 accesses}{}
OnNonBSpec (OnNonBSpeculative) does 4.04\% less L1 accesses then OnExectue. ReExecute decreases the L1 accesses with 3.60\% used on OnExecute and 0.95\% used on OnNonBSpec. The difference can be explained by OnNonBSpec removing some of the prefetches that ReExecute also will remove. The total numbers tell us that OnNonBSpec with Re-Execute as the lest number of L1 accesses apart from OnCommit with 100.00\% against 118.26\% (OnExectue with Re-Execute). 
\fullTable{L12}{run2-1-full}{Number of L1 accesses}
\toplist{L12}{run2-1}{number of L1 accesses }{OnNonBSpec with ReExecute}
The table above is pretty similar to table \ref{top:topL1_1} were OnCommit and OnExecute was compared. In this table you can see that OnBSpec has better values then OnCommit for all but two benchmarks with up to over 90 percent units. 
\resSp
\avgTable{sp2}{run2-2-avg}{number of store prefetches}{}
OnNonBSpec decreases the number of store prefetch by 57.60\% compared with OnExecute. Re-execute is also decreasing the number of prefetches with 15.64\% used on OnExecute and 7.68 used on OnNonBSpec. The difference can be explained by OnNonBSpec removing some of the prefetches that ReExecute also will remove. The total numbers tell us that OnNonBSpec with Re-execute has the lowest number of L1 accesses apart from OnCommit (and NoPrefetch) with 336.54 \% against 384.08 \% (OnExectue with Re-Execute).  
\fullTable{sp2}{run2-2-full}{Number of store prefetches}
\toplist{sp2}{run2-2}{Reduce speculation the benchmarks which number of store prefetches are affected the most (in number of percent)}{OnNonBSpec with ReExecute}
This table is very much alike table \ref{top:topsp1} were OnCommit, and OnExecute was compared. The benchmarks in both the top and bottom five are the same and lay all at the same place when comparing with the table above. The values differ some, some values are bit more and some a bit less compared to the other table.
\resEnergy 
\avgTable{energy2}{run2-3-avg}{Energy consumption}{}
What to notice of here is that OnNonBSpec burns 22.89\% less energy then OnExecute. Re-Execute decrease the energy with 3.24\% used on OnExecute and 0.82 \% used on OnNonBSpec. OnNonSPec with ReExecute gets 109.42\%, which is 9.42\% more then OnCommit.  
\fullTable{energy2}{run2-3-full}{Energy consumption}
\toplist{energy2}{run2-3}{energy consumption}{OnNonBSpec with ReExecute}
By comparing the above table with the list that consisting of comparison between OnCommit and OnExecute, we can see that all benchmarks in the table here have lower digits which does not surprise since OnCommit over all burns more energy then OnNonBSpec with ReExecute.

\subsection{Conclusion}
OnNonBSpec with Re-Execute is the fastest store prefetch policy with a speed-up of 3.42 percent compared to OnCommit, but it burns 9.42 percent more energy. The question to ask is if an energy increase with 2.75 \% is worth a speed-up of 1 \%. It is likely to believe that the answer will differ with the type of machine and application. OnNonBSpec with Re-Execute will be kept gathered to gather with the three basic policies as the reference for the future once. The new filters and policies to be tested will all be put on top of it.

\THsec{Techniques to filter unnecessary prefetches}{filter}
In this section sameCachLine and PCbasedPredictor (PCbased) are introduced. The digit after PCNessery denotes the buffer size, the number of entries is two to the power of the digit. Tests have been conducted with PCNessery 2, 4, 6, 8, 10 and 16 but as it turns out 2 behaves in the same way as 4, and 8 the same as 6, and so on in all four categories (execution time, L1 accesses, number of prefetch and energy consumption). Therefore, to reduce the number of configurations in the graphs of this section, we only use buffer sizes 2 and 8. The new configurations, based upon OnNonBSPec with Re-Execute, are:
\begin{itemize}
    \item OnNonBSpec with sameCacheLine
    \item PCbasedPredictor 4
    \item PCbasedPredictor 4 with sameCacheLine
    \item PCbasedPredictor 8
    \item PCbasedPredictor 8 with sameCacheLine
\end{itemize}
\resExtime
\avgTable{extime3}{run3-0-avg}{execution time}{* with sameCacheLine}
SamecacheLine has a little (slow down with 0,06\%) impact on OnNonBSpec (the best from \ref{sec:reduce}) and 0.08\%, on PCbasedPredictor (2 and 8). One hypothesis for this is that loading the same permissions twice in a short manner of time. This results in granting the permission the second time. Since the cache line is all ready in the L1 cache and the overhead to come to that conclusion is not that big. Therefore blocking the second prefetch of a cache line a bit earlier will not have a big impact, at least on the execution time. 
\\ \\ 
PCbased 4 and 8 have the same numbers 96.96\% without sameCacheLine and 96.88\% with. This results can be compared to OnNonBSpec with its 96.64\%, with sameCacheLine. Iterate over a buffer as in PCNessery takes time, so the advantage of doing that has to pay more at runtime. Otherwise, it will end up with a slowdown as here. 

\fullTable{extime3}{run3-0-full}{Execution time}
\toplist{extime3}{run3-0}{execution times}{OnNonBSpec with SameChacheLine}
Here we are a comparing between OnCommit and OnNonBSpec with ReExecute together with SameChachLine, since its the best one concerning execution time among the new one for this section. Most numbers in both the top five and the lowest five are higher in this table than in the one with a comparison between OnNonBSpec with (only) Re-Execute and OnCommit. This result tells us that SameChacheLine can have a negative impact on the runtime for a few benchmarks, but it improves on the majority of the benchmarks. The improvement on the improved once is smaller than the worse for, the worse once. 
\resAcc
\avgTable{L13}{run3-1-avg}{number of L1 accesses}{* with sameCacheLine}
PCbasedPredictor 4 and 8 have the same number of L1 accesses. SamecacheLIne improves both on OnNonBSpec with 1.15\% and on PCbasedPredictor (4 and 8) with 0.73\%. PCbasedPredictor (4 and 8) with SamecacheeLine becomes the closest one to OnCommit with its 105.03\%.
\fullTable{L13}{run3-1-full}{number of L1 accesses}
\toplist{L13}{run3-1}{Reduce speculation the benchmarks which number of L1 accesses are affected the most (in number of percent)}{OnNonBSpec with SameCaceLine}

\resSp
\avgTable{sp3}{run3-2-avg}{number of store prefetches}{* with SameCacheLine}
The Store prefetch shows roughly the same thing as L1 accsesses \ref{tab:avgL13} . PCNessary (4 and 8) gives the same percentage without SameChaheLine, 155.16\%, and with, 151.95\%, a difference of 3.21\%. SamecacheLine also improves OnNonBespec with 6.02\%. PCNessary (4 and 8) with SamecacheLine is once again closest to OnCommit with its 151.95\%.   
\fullTable{sp3}{run3-2-full}{Number of store prefetches}
\toplist{sp3}{run3-2}{number of store prefetches}{OnNonBSpec with SameChacheLine}
\resEnergy
\avgTable{energy3}{run3-3-avg}{energy consumption}{* with sameCacheLine}
There is no suprise that the energy comparasion looks the same as L1 accsess and prefetches. PCNessary (4 and 8) has the same precentages, without, 103.91\%, and with SamecacheLine and it is decressed with 0.66\% to 103.25\%. OnNonBSpec drops with 0.78\% to 108.64\% with SamecacheLine, however it is not enought to beat PCNessery with SamecacheLine.
\fullTable{energy3}{run3-3-full}{Energy consumption}
\toplist{energy3}{run3-3}{energy consumption}{OnNonBSpec with SameCacheLine}

\subsection{Conclusion}
SamecacheLine does not affect at all on the execution time, but it saves energy by removing L1 accesses and prefetches. Therefore SameCachLine should be added to the currently best policies. PCbasedPredictor gives the same runtime and burns the same amount of energy for both sizes 4 and 8. It is cheaper and more efficient to have a buffer with $2^4=16$ entries then $2^8=256$ entries and therefore we will look more into how PCNessery 4 does compare to OnNonBSpec, both with SameCacheLine.

    \begin{table}[H]
\centering

\begin{tabular}{ |l|l|l| }
\cline{2-3}
\multicolumn{1}{ c| }{} 
& Execution time & Energy  \\  \hline
PCNessery 4& 96.88 & 103.25  \\  \hline
OnNonBSPec & 96.64 & 108.64  \\  \hline
Difference & 0.24 & 5.39  \\  \hline
\end{tabular}
\caption{Comparation between PCNessery 4 and OnNonBSpec, both with SamecacheLine. }
\label{quickcompPC4vsONBS}
\end{table}
In the table above \ref{quickcompPC4vsONBS} we see that  OnNonBSpec is fastest but PCNessary 4 burns less energy. So the choice is based on what the biggest concern might be in a given setting, time or energy. PCbasedPredictor 4 is a bit slower, 0.24\% the energy savings is mush higher therefore should PCNessary 4 with SamecacheLine be considered the best policy of this section and be together with the basic three the once to be taken to the next section. 

\THsec{Techniques for timelLiness}{timelines}
In this last section we have added the following policies which all build upon PCbasedPredictor 4 with Re-Execute and SamecacheLine:
\begin{itemize}
    \item PCbasedPredictorTimeliness2 4 (PCbasedTL2 4)
    \item PCbasedPredictorTimeliness2 8 (PCbasedTL2 8)
    \item PCbasedPredictorTimeliness3 4 (PCbasedTL3 4)
    \item PCbasedPredictorTimeliness3 8 (PCbasedTL3 8)
\end{itemize}
\resExtime

\avgTable{extime4}{run4-0-avg}{execution time}{* with reExecute and SamecacheLine}
PCNessaryTimeliness2 (4 and 8) has the same runtime as PCNessary 4, 96.88\%. PCNessaryTimeliness3 has longer runtimes, 97.89\% for size 4 and 97.51\% for size 8, itirate over a bigger buffer takes more time for PCNessaryTimeliness3.
\fullTable{extime4}{run4-0-full}{Execution time}
\toplist{extime4}{run4-0}{execution time}{PCbasedPredictor~4*}
\resAcc
\avgTable{L14}{run4-1-avg}{number of L1 accesses}{* with reExecute and SamecacheLine}
PCNessaryTimeliness2 (4 and 8) gets the same number of L1 accesses as PCNessary 4, 105.03\%. PCNessaryTimeliness3 (4 and 8) are getting more or less the same precentages, (98.36\% and 98.34\%), pretty close to OnCommit, 100.0\%
\fullTable{L14}{run4-1-full}{Number of L1 accesses}
\toplist{L14}{run4-1}{number of L1 accesses}{PCbasedPredictor~4*}
\resSp
\avgTable{sp4}{run4-2-avg}{number of store prefetches}{* with reExecute and SamecacheLine}
Once again PCNessaryTimeliness2 (4 and 8) gets the same number of L1 accesses as PCNessary 4, 151.95\% and PCNessaryTimeliness3 lays close to OnCommit, (99.60\% for size 4 and 98.89\% for size 8).
\fullTable{sp4}{run4-2-full}{Number of store prefetches}
\toplist{sp4}{run4-2}{TimelLiness: the benchmarks which number of store prefetches are effected the most (in number of percent)}{PCbasedPredictor~4*}
\resEnergy
\avgTable{energy4}{run4-3-avg}{energy consumption}{* with reExecute and SamecacheLine}
Once again PCNessaryTimeliness2 (4 and 8) gets the same energy consumption as PCNessary 4, 103.25\%. However PCNessaryTimeliness3 8 consumes 5.09\% less energy than OnCommit and is therefore the prefetch policy that burns less energy (except NoPrefetech) with its 98.16\%.     PCNessaryTimeliness3 4 consumes slightly more energy than OnCommit,  100.08\%.
  
\fullTable{energy4}{run4-3-full}{Energy consumption}
\toplist{energy4}{run4-3}{energy consumption}{PCbasedPredictor~4*}


\subsection{Conclusion}
PCNessaryTimeliness2 (4 and 8) are out of the game since they perform the same as PCbasedPredictor 4 in all measured aspects. PCNessaryTimeliness3 8 is interesting since it consumes less energy then OnCommit and is also faster. It is 2.49\% slower then PCbasedPredictor 4, but it burns 1.84\% more energy. Therefore PCNessaryTimeliness3 8 is the best prefetch policy proposed in this thesis.